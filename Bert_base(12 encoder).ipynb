{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7cf0677-d09c-479a-9ee2-0629f7be7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8e97c54-4d15-4dae-b3f5-328fd276cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Ruban is working on BERT models.\",\n",
    "    \"This sentence contains multiple types of punctuation.\",\n",
    "    \"How can BERT be fine-tuned for specific tasks?\",\n",
    "    \"What is the current project you are working on?\",\n",
    "    \"How does BERT handle different sentence structures?\",\n",
    "    \"Ruban is learning deep learning techniques.\",\n",
    "    \"BERT is a powerful model for NLP tasks.\",\n",
    "    \"Understanding attention mechanisms is crucial.\"\n",
    "]\n",
    "\n",
    "tokens = [sentence.lower().split() for sentence in sentences]\n",
    "tokens_flat = [token for sublist in tokens for token in sublist]\n",
    "\n",
    "vocab = Counter(tokens_flat)\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(vocab.most_common())}  \n",
    "vocab['[PAD]'] = 0\n",
    "vocab['[UNK]'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21772fea-baa5-468c-9431-c6259b44c95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and Padded Sentences:\n",
      " tensor([[ 4,  2,  5, 10,  3, 11,  0,  0,  0],\n",
      "        [12,  6, 13, 14, 15, 16, 17,  0,  0],\n",
      "        [ 7, 18,  3, 19, 20,  8, 21, 22,  0],\n",
      "        [23,  2, 24, 25, 26, 27, 28,  5, 29],\n",
      "        [ 7, 30,  3, 31, 32,  6, 33,  0,  0],\n",
      "        [ 4,  2,  9, 34,  9, 35,  0,  0,  0],\n",
      "        [ 3,  2, 36, 37, 38,  8, 39, 40,  0],\n",
      "        [41, 42, 43,  2, 44,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence(sentence, vocab):\n",
    "    tokens = sentence.lower().split()\n",
    "    token_ids = [vocab.get(token, vocab['[UNK]']) for token in tokens]\n",
    "    return token_ids\n",
    "\n",
    "tokenized_sentences = [tokenize_sentence(sentence, vocab) for sentence in sentences]\n",
    "max_len = max(len(s) for s in tokenized_sentences)\n",
    "padded_sentences = [s + [vocab['[PAD]']] * (max_len - len(s)) for s in tokenized_sentences]\n",
    "\n",
    "input_ids = torch.tensor(padded_sentences, dtype=torch.long)\n",
    "print(\"Tokenized and Padded Sentences:\\n\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e1d9e9f-66e0-452f-adee-350da13cdd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Shape: torch.Size([9, 768])\n"
     ]
    }
   ],
   "source": [
    "def get_positional_encoding(seq_len, model_dim):\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(model_dim)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(model_dim))\n",
    "    angle_rads = pos * angle_rates\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return torch.tensor(angle_rads, dtype=torch.float32)\n",
    "\n",
    "model_dim = 768\n",
    "positional_encoding = get_positional_encoding(max_len, model_dim)\n",
    "print(\"Positional Encoding Shape:\", positional_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abb452ca-5c13-4941-be40-9a947c95de0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Input Shape: torch.Size([8, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = model_dim\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "embedded = embedding_layer(input_ids)\n",
    "print(\"Embedded Input Shape:\", embedded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0ba64197-9d01-4c29-9b8b-b033af40a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output Shape: torch.Size([8, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "class BERTSelfAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super(BERTSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        assert model_dim % num_heads == 0\n",
    "        self.depth = model_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(model_dim, model_dim)\n",
    "        self.key = nn.Linear(model_dim, model_dim)\n",
    "        self.value = nn.Linear(model_dim, model_dim)\n",
    "        self.out = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        query = self.split_heads(self.query(x), batch_size)\n",
    "        key = self.split_heads(self.key(x), batch_size)\n",
    "        value = self.split_heads(self.value(x), batch_size)\n",
    "        \n",
    "        score = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
    "        attention_weights = torch.nn.functional.softmax(score, dim=-1)\n",
    "        \n",
    "        attention_output = torch.matmul(attention_weights, value)\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(batch_size, -1, self.model_dim)\n",
    "        \n",
    "        output = self.out(attention_output)\n",
    "        return output\n",
    "\n",
    "class BERTLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super(BERTLayer, self).__init__()\n",
    "        self.attention = BERTSelfAttention(model_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(model_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, model_dim)\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(x)\n",
    "        out1 = self.layernorm1(x + self.dropout(attention_output))\n",
    "        ff_output = self.ff(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout(ff_output))\n",
    "        return out2\n",
    "\n",
    "class BERTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, num_heads, num_layers, ff_hidden_dim, max_len):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, model_dim)\n",
    "        self.positional_encoding = get_positional_encoding(max_len, model_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            BERTLayer(model_dim, num_heads, ff_hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids) + self.positional_encoding\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = BERTModel(vocab_size, model_dim, num_heads=12, num_layers=12, ff_hidden_dim=3072, max_len=max_len)\n",
    "output = model(input_ids)\n",
    "print(\"Final Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0ce67c3-9cc7-47e2-bd0f-8c9b20cfc62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after layer 1: tensor([[ 0.3898,  0.5658, -0.4369,  ..., -0.1406, -0.7689,  0.0873],\n",
      "        [ 1.1101,  0.1013,  0.2335,  ...,  0.5518, -1.4077, -0.3422],\n",
      "        [-0.0110, -0.8820,  0.5176,  ...,  1.0359, -0.3034,  1.1285],\n",
      "        ...,\n",
      "        [-1.2283,  1.4392, -0.2876,  ...,  0.7713, -0.1409,  2.4968],\n",
      "        [-0.4554,  1.2451,  0.1158,  ...,  0.6447, -0.0852,  2.4578],\n",
      "        [-0.1555,  0.5083,  0.5224,  ...,  0.5783, -0.0377,  2.3370]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 2: tensor([[ 0.4991,  0.9400, -0.4286,  ..., -0.1892, -0.6020, -0.3038],\n",
      "        [ 1.5099,  0.2616, -0.0982,  ...,  0.5521, -1.6127, -0.6876],\n",
      "        [ 0.2074, -0.3140,  0.4856,  ...,  0.8999, -0.9438,  0.5205],\n",
      "        ...,\n",
      "        [-1.0810,  1.7156, -0.2261,  ...,  0.7791, -0.5467,  2.0269],\n",
      "        [-0.4311,  1.4270,  0.1953,  ...,  0.7361, -0.5516,  2.2859],\n",
      "        [-0.1050,  0.8034,  0.6049,  ...,  0.6172, -0.5052,  1.8046]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 3: tensor([[ 0.7923,  0.9567, -0.3744,  ..., -0.2832, -0.5505, -0.3027],\n",
      "        [ 1.2576,  0.1255, -0.1940,  ...,  0.5466, -1.5596, -1.0828],\n",
      "        [ 0.1777, -0.0472,  0.3668,  ...,  1.1016, -1.0427,  0.3267],\n",
      "        ...,\n",
      "        [-0.5795,  2.1641, -0.6283,  ...,  0.8599, -0.6725,  1.6747],\n",
      "        [-0.3171,  1.7646, -0.2830,  ...,  0.9243, -0.6491,  1.8653],\n",
      "        [ 0.5040,  1.2081,  0.1276,  ...,  0.7904, -0.6077,  1.5398]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 4: tensor([[ 0.3009,  1.0131, -0.7385,  ..., -1.0532, -0.4204, -0.4521],\n",
      "        [ 0.9936,  0.4296, -0.4829,  ..., -0.2154, -1.3480, -1.2272],\n",
      "        [ 0.3398,  0.1703,  0.3129,  ...,  0.5933, -0.5318,  0.1389],\n",
      "        ...,\n",
      "        [-0.8927,  2.4928, -0.5996,  ...,  0.4030, -0.3695,  1.3836],\n",
      "        [-0.5576,  2.0939, -0.2904,  ...,  0.5388, -0.2602,  1.5664],\n",
      "        [ 0.2545,  1.4955,  0.0074,  ...,  0.3799, -0.4656,  1.3456]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 5: tensor([[-0.1247,  0.1286, -0.8996,  ..., -0.4066, -0.1683, -0.1837],\n",
      "        [ 0.4875, -0.1827, -0.5332,  ...,  0.0846, -1.0663, -0.9298],\n",
      "        [ 0.2530, -0.3495,  0.0274,  ...,  1.1875, -0.2246,  0.2080],\n",
      "        ...,\n",
      "        [-0.9003,  1.9245, -0.5572,  ...,  0.4487,  0.0699,  1.6883],\n",
      "        [-0.4141,  1.5659, -0.3027,  ...,  0.6376,  0.3232,  1.8375],\n",
      "        [ 0.3415,  0.9995, -0.0941,  ...,  0.5314,  0.0454,  1.6893]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 6: tensor([[ 0.0892,  0.3964, -0.5963,  ..., -0.5917, -0.3634, -0.5037],\n",
      "        [ 0.7073, -0.0860, -0.4691,  ..., -0.2246, -1.2711, -1.3211],\n",
      "        [ 0.2950, -0.3337,  0.2065,  ...,  0.8461, -1.1158, -0.2803],\n",
      "        ...,\n",
      "        [-0.8602,  1.9654, -0.2810,  ..., -0.1478, -0.2716,  1.1791],\n",
      "        [-0.3471,  1.6179, -0.1341,  ..., -0.0652,  0.0230,  1.2530],\n",
      "        [ 0.4968,  1.1073,  0.0540,  ..., -0.0598, -0.3164,  1.2645]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 7: tensor([[ 0.1382,  0.4204, -1.0597,  ..., -0.6648, -0.2299, -0.0507],\n",
      "        [ 0.7339, -0.1775, -0.8208,  ...,  0.2840, -1.3959, -0.7228],\n",
      "        [ 0.2578, -0.3780, -0.3473,  ...,  0.6752, -1.2073, -0.1472],\n",
      "        ...,\n",
      "        [-0.9515,  1.7508, -0.7287,  ..., -0.1554, -0.5901,  1.5428],\n",
      "        [-0.5014,  1.2899, -0.4635,  ...,  0.0157, -0.3898,  1.7019],\n",
      "        [ 0.1937,  0.8307, -0.0877,  ...,  0.0133, -0.7566,  1.5475]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 8: tensor([[ 0.0765, -0.0314, -0.9827,  ..., -0.6996, -0.0331, -0.2117],\n",
      "        [ 0.8959, -0.4883, -0.9554,  ..., -0.3110, -0.8205, -0.7913],\n",
      "        [ 0.6345, -0.6384, -0.4219,  ...,  0.4393, -0.6947, -0.0745],\n",
      "        ...,\n",
      "        [-0.8460,  1.7479, -0.6291,  ..., -0.6079, -0.2400,  1.5485],\n",
      "        [-0.4490,  1.1992, -0.4802,  ..., -0.4419, -0.0767,  1.7906],\n",
      "        [ 0.2502,  0.8476, -0.0099,  ..., -0.6359, -0.3486,  1.7260]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 9: tensor([[ 0.1973,  0.5624, -1.0773,  ..., -1.0943,  0.1487,  0.1089],\n",
      "        [ 0.8458,  0.1559, -0.8170,  ..., -0.9812, -0.1736, -0.3664],\n",
      "        [ 0.8717,  0.0428, -0.2385,  ..., -0.1619, -0.5045,  0.4080],\n",
      "        ...,\n",
      "        [-0.5417,  2.0378, -0.4818,  ..., -0.8374,  0.1274,  1.7690],\n",
      "        [-0.1748,  1.3708, -0.1522,  ..., -0.5461,  0.1284,  2.0259],\n",
      "        [ 0.4279,  1.0759,  0.0900,  ..., -0.8509, -0.1964,  1.9529]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 10: tensor([[ 0.1085,  0.9792, -1.1569,  ..., -0.9188,  0.4359,  0.1560],\n",
      "        [ 1.1929,  0.1448, -0.9275,  ..., -0.5905, -0.1780, -0.3859],\n",
      "        [ 0.9117, -0.1368, -0.1897,  ..., -0.1363, -0.3065,  0.2002],\n",
      "        ...,\n",
      "        [-0.3775,  1.8682, -0.4410,  ..., -0.6936, -0.0131,  1.7905],\n",
      "        [-0.1049,  1.1867, -0.2489,  ..., -0.2572,  0.0659,  2.1112],\n",
      "        [ 0.4537,  0.8257, -0.0080,  ..., -0.8886, -0.2518,  2.0298]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 11: tensor([[ 0.3274,  1.2409, -1.3451,  ..., -1.3249,  0.0998, -0.7890],\n",
      "        [ 1.0864,  0.3874, -0.9189,  ..., -1.3399, -0.9161, -0.6343],\n",
      "        [ 0.9126,  0.0368, -0.1413,  ..., -0.7472, -0.5636, -0.4273],\n",
      "        ...,\n",
      "        [-0.6031,  2.1537, -0.4003,  ..., -1.2395, -0.3437,  0.8856],\n",
      "        [-0.2623,  1.6051, -0.2773,  ..., -0.6849, -0.4118,  1.2305],\n",
      "        [ 0.5728,  0.7565, -0.0947,  ..., -1.3460, -0.8053,  1.2220]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Output after layer 12: tensor([[ 0.6740,  1.2382, -1.5957,  ..., -0.9930, -0.0191, -1.0486],\n",
      "        [ 1.1878, -0.0375, -1.3705,  ..., -1.0919, -0.9016, -1.1198],\n",
      "        [ 1.1952, -0.0948, -0.7378,  ..., -0.4654, -0.3540, -0.8166],\n",
      "        ...,\n",
      "        [-0.4242,  1.8563, -0.6429,  ..., -1.1976, -0.2317,  0.4095],\n",
      "        [-0.2003,  1.1951, -0.7028,  ..., -0.5361, -0.3896,  0.8181],\n",
      "        [ 0.4907,  0.6378, -0.4248,  ..., -1.0239, -0.8109,  1.1064]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = embedding_layer(input_ids) + positional_encoding\n",
    "for i, layer in enumerate(model.layers):\n",
    "    x = layer(x)\n",
    "    print(f\"Output after layer {i+1}:\", x[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404961a-5b4c-42ff-9d69-d2381e077318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
