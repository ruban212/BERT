{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a5deaef-e65a-423f-b4df-cd50a172608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd713b74-8e93-4f94-b018-bb901c713cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sentence\n",
    "input_sentence = \"Good morning Ruban, How are you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "385389cc-b71f-4ad4-955e-00b15384bcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Good', 'morning', 'Ruban,', 'How', 'are', 'you']\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "def tokenize(sentence):\n",
    "    return sentence.split()\n",
    "\n",
    "tokens = tokenize(input_sentence)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f03f83c8-16d5-4962-930e-1120dd77c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Vectors:\n",
      " [[0.96747725 0.17564349 0.30792887 0.80851704 0.29674353 0.09708126\n",
      "  0.9429399  0.12252473 0.51040979 0.64163922 0.16073098 0.41539569\n",
      "  0.32313807 0.53614944 0.75662873 0.51511293 0.33922534 0.32840459\n",
      "  0.60401629 0.54554346 0.90795429 0.58322242 0.39796271 0.07869423\n",
      "  0.23236048 0.8850849  0.57045839 0.6513685  0.38900614 0.33679803\n",
      "  0.49462681 0.96331206 0.22620105 0.81278437 0.21725907 0.51002877\n",
      "  0.18382424 0.49728352 0.55430084 0.86259964 0.36448245 0.82149816\n",
      "  0.61989899 0.95966647 0.94040518 0.22373659 0.41271711 0.80968164\n",
      "  0.34369479 0.39578817]\n",
      " [0.60141179 0.73993431 0.70459804 0.11710499 0.1363244  0.43395856\n",
      "  0.85490611 0.30819253 0.84827225 0.75261964 0.64829959 0.35222475\n",
      "  0.7312547  0.6266549  0.06933976 0.66422488 0.58803975 0.25123219\n",
      "  0.39939021 0.96235546 0.75451498 0.1077278  0.51227802 0.67949022\n",
      "  0.61183421 0.10602705 0.57297972 0.3238125  0.70651491 0.66608476\n",
      "  0.99577213 0.66842767 0.68002159 0.66413475 0.36464203 0.7703872\n",
      "  0.37752045 0.62867142 0.7072148  0.82271647 0.21006119 0.19621425\n",
      "  0.14538674 0.55241292 0.49771231 0.39012578 0.00678276 0.11728383\n",
      "  0.72596694 0.93924674]\n",
      " [0.11961123 0.4585521  0.34004303 0.86728817 0.41229206 0.58449605\n",
      "  0.71141331 0.24747674 0.51264072 0.39029713 0.3728561  0.28844323\n",
      "  0.00585658 0.33650046 0.28631635 0.76576955 0.82611546 0.32206774\n",
      "  0.3522144  0.34760336 0.52775505 0.6064878  0.17222398 0.4947939\n",
      "  0.48453425 0.99154295 0.40595008 0.9580752  0.4076801  0.07179509\n",
      "  0.91661749 0.08528096 0.72088756 0.14668387 0.22867943 0.77817424\n",
      "  0.81725245 0.96563621 0.1192632  0.1914057  0.19536781 0.66829235\n",
      "  0.6095913  0.11954901 0.13900687 0.80234311 0.83812515 0.59142249\n",
      "  0.05594353 0.21706438]\n",
      " [0.88090877 0.51379413 0.99734319 0.40144909 0.51217035 0.12831977\n",
      "  0.13080707 0.88806963 0.05453536 0.85794623 0.29732461 0.13687473\n",
      "  0.64821405 0.75458396 0.71160674 0.6092238  0.15214655 0.85496512\n",
      "  0.8790429  0.172193   0.47945888 0.45627278 0.18563175 0.99752289\n",
      "  0.49204768 0.36047705 0.85725656 0.91575074 0.50251213 0.27824779\n",
      "  0.5430181  0.3636639  0.56969842 0.2409452  0.95576396 0.49611807\n",
      "  0.69412412 0.54914424 0.78822043 0.44525834 0.40535235 0.57678466\n",
      "  0.54085153 0.0740679  0.89090148 0.40167289 0.28533601 0.10769948\n",
      "  0.80586754 0.38125393]\n",
      " [0.33775826 0.9984142  0.95379911 0.87270338 0.57403722 0.47346074\n",
      "  0.08645809 0.85672055 0.27386888 0.12247817 0.91315011 0.88655406\n",
      "  0.54449165 0.38705922 0.92923346 0.66182895 0.36129452 0.19920209\n",
      "  0.40393142 0.77971999 0.79394699 0.83146175 0.93908418 0.60473721\n",
      "  0.41983063 0.76886251 0.51254959 0.02516663 0.92302531 0.0063783\n",
      "  0.23417451 0.31737819 0.94577608 0.94432719 0.32418103 0.4784035\n",
      "  0.55277798 0.82746363 0.05289772 0.58593776 0.34729756 0.18207385\n",
      "  0.96955984 0.24338733 0.25730011 0.43149029 0.58269788 0.12195287\n",
      "  0.0801581  0.4595561 ]\n",
      " [0.66546898 0.40307804 0.14060382 0.30165586 0.87258126 0.94120871\n",
      "  0.17015359 0.27306264 0.6042768  0.66039749 0.91485196 0.09733602\n",
      "  0.13468817 0.95242601 0.17486425 0.70395059 0.86196287 0.96588601\n",
      "  0.84424568 0.89073235 0.08324824 0.79669384 0.71910834 0.33897074\n",
      "  0.43571444 0.72557016 0.00355312 0.99599906 0.56729004 0.94960658\n",
      "  0.61392485 0.7185179  0.29632462 0.98756535 0.15971478 0.81979543\n",
      "  0.62914143 0.77138221 0.97249792 0.17149012 0.22915007 0.34022303\n",
      "  0.39302275 0.37843185 0.53641153 0.07726411 0.783315   0.1285433\n",
      "  0.72298599 0.38681087]]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to vectors\n",
    "def tokens_to_vectors(tokens, vocab_size=1000, embedding_dim=50):\n",
    "    token_indices = np.random.randint(0, vocab_size, size=len(tokens))\n",
    "    embeddings = np.random.rand(vocab_size, embedding_dim)\n",
    "    return embeddings[token_indices]\n",
    "\n",
    "token_vectors = tokens_to_vectors(tokens)\n",
    "print(\"Token Vectors:\\n\", token_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b8e8777-04f7-48f0-8953-703f4aa0aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding:\n",
      " [[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00]\n",
      " [ 8.41470985e-01  6.73573169e-01  6.37948243e-01  8.38952968e-01\n",
      "   4.60563641e-01  9.21796446e-01  3.25112910e-01  9.62310282e-01\n",
      "   2.27088243e-01  9.81900958e-01  1.57826640e-01  9.91323572e-01\n",
      "   1.09428242e-01  9.95844063e-01  7.57850258e-02  9.98010124e-01\n",
      "   5.24566587e-02  9.99047421e-01  3.62998288e-02  9.99544029e-01\n",
      "   2.51162229e-02  9.99781750e-01  1.73771336e-02  9.99895537e-01\n",
      "   1.20223547e-02  9.99950000e-01  8.31754180e-03  9.99976069e-01\n",
      "   5.75436762e-03  9.99988546e-01  3.98106119e-03  9.99994518e-01\n",
      "   2.75422522e-03  9.99997376e-01  1.90545956e-03  9.99998744e-01\n",
      "   1.31825636e-03  9.99999399e-01  9.12010713e-04  9.99999712e-01\n",
      "   6.30957303e-04  9.99999862e-01  4.36515818e-04  9.99999934e-01\n",
      "   3.01995167e-04  9.99999968e-01  2.08929612e-04  9.99999985e-01\n",
      "   1.44543977e-04  9.99999993e-01]\n",
      " [ 9.09297427e-01 -9.25983720e-02  9.82541399e-01  4.07684165e-01\n",
      "   8.17617157e-01  6.99417376e-01  6.14902436e-01  8.52082159e-01\n",
      "   4.42310755e-01  9.28258983e-01  3.11697146e-01  9.65444848e-01\n",
      "   2.17542185e-01  9.83410796e-01  1.51134163e-01  9.92048417e-01\n",
      "   1.04768873e-01  9.96191498e-01  7.25518104e-02  9.98176533e-01\n",
      "   5.02165994e-02  9.99127095e-01  3.47490196e-02  9.99582170e-01\n",
      "   2.40429717e-02  9.99800007e-01  1.66345082e-02  9.99904276e-01\n",
      "   1.15085447e-02  9.99954183e-01  7.96205928e-03  9.99978071e-01\n",
      "   5.50842955e-03  9.99989504e-01  3.81091221e-03  9.99994976e-01\n",
      "   2.63651042e-03  9.99997595e-01  1.82402067e-03  9.99998849e-01\n",
      "   1.26191435e-03  9.99999449e-01  8.73031554e-04  9.99999736e-01\n",
      "   6.03990307e-04  9.99999874e-01  4.17859214e-04  9.99999940e-01\n",
      "   2.89087950e-04  9.99999971e-01]\n",
      " [ 1.41120008e-01 -7.98316727e-01  8.75321228e-01 -1.54897288e-01\n",
      "   9.90913972e-01  3.67644457e-01  8.37883066e-01  6.77624564e-01\n",
      "   6.34421810e-01  8.41015812e-01  4.57754548e-01  9.22812898e-01\n",
      "   3.23043315e-01  9.62803541e-01  2.25614032e-01  9.82138604e-01\n",
      "   1.56792596e-01  9.91437674e-01  1.08708161e-01  9.95898758e-01\n",
      "   7.52852930e-02  9.98036322e-01  5.21104117e-02  9.99059964e-01\n",
      "   3.60601134e-02  9.99550034e-01  2.49503237e-02  9.99784624e-01\n",
      "   1.72623407e-02  9.99896913e-01  1.19429312e-02  9.99950659e-01\n",
      "   8.26259209e-03  9.99976384e-01  5.71635102e-03  9.99988697e-01\n",
      "   3.95475991e-03  9.99994590e-01  2.73602910e-03  9.99997411e-01\n",
      "   1.89287090e-03  9.99998761e-01  1.30954712e-03  9.99999407e-01\n",
      "   9.05985392e-04  9.99999716e-01  6.26788798e-04  9.99999864e-01\n",
      "   4.33631918e-04  9.99999935e-01]\n",
      " [-7.56802495e-01 -9.82851083e-01  3.65592016e-01 -6.67587243e-01\n",
      "   9.41506211e-01 -2.16306683e-02  9.69828047e-01  4.52088013e-01\n",
      "   7.93383293e-01  7.23329481e-01  5.92337725e-01  8.64167508e-01\n",
      "   4.24664500e-01  9.34193586e-01  2.98796249e-01  9.68320123e-01\n",
      "   2.08384575e-01  9.84795003e-01  1.44721221e-01  9.92712781e-01\n",
      "   1.00306487e-01  9.96509905e-01  6.94560672e-02  9.98329029e-01\n",
      "   4.80720430e-02  9.99200107e-01  3.32644132e-02  9.99617120e-01\n",
      "   2.30155651e-02  9.99816736e-01  1.59236138e-02  9.99912283e-01\n",
      "   1.10166920e-02  9.99958016e-01  7.62176908e-03  9.99979905e-01\n",
      "   5.27300252e-03  9.99990382e-01  3.64803527e-03  9.99995396e-01\n",
      "   2.52382670e-03  9.99997797e-01  1.74606244e-03  9.99998945e-01\n",
      "   1.20798039e-03  9.99999495e-01  8.35718355e-04  9.99999758e-01\n",
      "   5.78175876e-04  9.99999884e-01]\n",
      " [-9.58924275e-01 -5.25727510e-01 -3.12251583e-01 -9.65251311e-01\n",
      "   6.80498103e-01 -4.07522603e-01  9.96401611e-01  1.92473322e-01\n",
      "   9.10889207e-01  5.79460008e-01  7.12073170e-01  7.90526343e-01\n",
      "   5.21185207e-01  8.97818730e-01  3.70259898e-01  9.50647969e-01\n",
      "   2.59402748e-01  9.76276143e-01  1.80543523e-01  9.88621509e-01\n",
      "   1.25264396e-01  9.94548512e-01  8.67807478e-02  9.97389516e-01\n",
      "   6.00770241e-02  9.98750260e-01  4.15762013e-02  9.99401772e-01\n",
      "   2.87680273e-02  9.99713655e-01  1.99040441e-02  9.99862943e-01\n",
      "   1.37707083e-02  9.99934400e-01  9.52715946e-03  9.99968602e-01\n",
      "   6.59123597e-03  9.99984972e-01  4.56003839e-03  9.99992807e-01\n",
      "   3.15478149e-03  9.99996557e-01  2.18257743e-03  9.99998352e-01\n",
      "   1.50997529e-03  9.99999211e-01  1.04464788e-03  9.99999623e-01\n",
      "   7.22719822e-04  9.99999819e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding\n",
    "def positional_encoding(seq_len, embedding_dim):\n",
    "    pos_enc = np.zeros((seq_len, embedding_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            pos_enc[pos, i] = np.sin(pos / (10000 ** (i / embedding_dim)))\n",
    "            pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((i + 1) / embedding_dim)))\n",
    "    return pos_enc\n",
    "\n",
    "pos_enc = positional_encoding(len(tokens), token_vectors.shape[-1])\n",
    "print(\"Positional Encoding:\\n\", pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5af2eaa-6e26-4576-8312-d00971051f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vectors (Tokens + Positional Encoding):\n",
      " [[ 0.96747725  1.17564349  0.30792887  1.80851704  0.29674353  1.09708126\n",
      "   0.9429399   1.12252473  0.51040979  1.64163922  0.16073098  1.41539569\n",
      "   0.32313807  1.53614944  0.75662873  1.51511293  0.33922534  1.32840459\n",
      "   0.60401629  1.54554346  0.90795429  1.58322242  0.39796271  1.07869423\n",
      "   0.23236048  1.8850849   0.57045839  1.6513685   0.38900614  1.33679803\n",
      "   0.49462681  1.96331206  0.22620105  1.81278437  0.21725907  1.51002877\n",
      "   0.18382424  1.49728352  0.55430084  1.86259964  0.36448245  1.82149816\n",
      "   0.61989899  1.95966647  0.94040518  1.22373659  0.41271711  1.80968164\n",
      "   0.34369479  1.39578817]\n",
      " [ 1.44288277  1.41350748  1.34254629  0.95605795  0.59688804  1.35575501\n",
      "   1.18001902  1.27050281  1.07536049  1.7345206   0.80612623  1.34354832\n",
      "   0.84068294  1.62249896  0.14512479  1.66223501  0.64049641  1.25027961\n",
      "   0.43569004  1.96189949  0.7796312   1.10750955  0.52965515  1.67938576\n",
      "   0.62385657  1.10597706  0.58129727  1.32378857  0.71226927  1.66607331\n",
      "   0.99975319  1.66842219  0.68277581  1.66413213  0.36654749  1.77038594\n",
      "   0.3788387   1.62867082  0.70812681  1.82271618  0.21069214  1.19621411\n",
      "   0.14582325  1.55241285  0.4980143   1.39012575  0.00699169  1.11728381\n",
      "   0.72611148  1.93924673]\n",
      " [ 1.02890866  0.36595373  1.32258443  1.27497234  1.22990922  1.28391342\n",
      "   1.32631574  1.0995589   0.95495147  1.31855611  0.68455324  1.25388808\n",
      "   0.22339877  1.31991126  0.43745052  1.75781796  0.93088433  1.31825924\n",
      "   0.42476621  1.34577989  0.57797165  1.60561489  0.20697299  1.49437607\n",
      "   0.50857722  1.99134296  0.42258459  1.95797947  0.41918865  1.07174927\n",
      "   0.92457955  1.08525903  0.72639599  1.14667337  0.23249034  1.77816922\n",
      "   0.81988896  1.96563381  0.12108722  1.19140455  0.19662973  1.66829179\n",
      "   0.61046433  1.11954875  0.13961086  1.80234298  0.83854301  1.59142243\n",
      "   0.05623261  1.21706435]\n",
      " [ 1.02202877 -0.2845226   1.87266442  0.24655181  1.50308432  0.49596423\n",
      "   0.96869014  1.5656942   0.68895717  1.69896204  0.75507916  1.05968763\n",
      "   0.97125737  1.7173875   0.93722077  1.5913624   0.30893915  1.8464028\n",
      "   0.98775106  1.16809176  0.55474417  1.4543091   0.23774217  1.99658285\n",
      "   0.5281078   1.36002708  0.88220688  1.91553536  0.51977447  1.2781447\n",
      "   0.55496103  1.36361456  0.57796102  1.24092158  0.96148031  1.49610677\n",
      "   0.69807888  1.54913883  0.79095646  1.44525575  0.40724522  1.57678342\n",
      "   0.54216108  1.07406731  0.89180746  1.40167261  0.2859628   1.10769934\n",
      "   0.80630117  1.38125386]\n",
      " [-0.41904424  0.01556312  1.31939112  0.20511614  1.51554343  0.45183007\n",
      "   1.05628614  1.30880856  1.06725217  0.84580765  1.50548784  1.75072157\n",
      "   0.96915615  1.32125281  1.22802971  1.63014908  0.5696791   1.18399709\n",
      "   0.54865264  1.77243277  0.89425348  1.82797166  1.00854025  1.60306624\n",
      "   0.46790268  1.76806262  0.545814    1.02478375  0.94604088  1.00619504\n",
      "   0.25009813  1.31729047  0.95679277  1.9442852   0.3318028   1.4783834\n",
      "   0.55805098  1.82745401  0.05654576  1.58593316  0.34982138  1.18207165\n",
      "   0.9713059   1.24338628  0.25850809  1.43148979  0.5835336   1.12195263\n",
      "   0.08073628  1.45955598]\n",
      " [-0.2934553  -0.12264947 -0.17164776 -0.66359545  1.55307936  0.53368611\n",
      "   1.1665552   0.46553596  1.51516601  1.2398575   1.62692513  0.88786236\n",
      "   0.65587338  1.85024474  0.54512415  1.65459856  1.12136562  1.94216215\n",
      "   1.02478921  1.87935386  0.20851263  1.79124235  0.80588908  1.33636026\n",
      "   0.49579147  1.72432042  0.04512932  1.99540083  0.59605807  1.94932023\n",
      "   0.6338289   1.71838084  0.31009533  1.98749975  0.16924194  1.81976404\n",
      "   0.63573267  1.77136718  0.97705796  1.17148293  0.23230485  1.34021958\n",
      "   0.39520533  1.3784302   0.5379215   1.07726332  0.78435965  1.12854292\n",
      "   0.72370871  1.38681068]]\n"
     ]
    }
   ],
   "source": [
    "input_vectors = token_vectors + pos_enc\n",
    "print(\"Input Vectors (Tokens + Positional Encoding):\\n\", input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af49f6b3-6272-4d3c-8882-0e1ba2bc1f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled dot prduct attention:\n",
      " (6, 50)\n"
     ]
    }
   ],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    matmul_qk = np.dot(Q, K.T)\n",
    "    dk = Q.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n",
    "    attention_weights = softmax(scaled_attention_logits, axis=-1)\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output, scaled_attention_logits.shape\n",
    "\n",
    "print(\"Scaled dot prduct attention:\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "114f9444-71de-404e-b44a-697972e1dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x, axis=None):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "# Layer Normalization\n",
    "def layer_norm(x, epsilon=1e-6):\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    std = np.std(x, axis=-1, keepdims=True)\n",
    "    return (x - mean) / (std + epsilon)\n",
    "\n",
    "# Feed Forward Network(ReLU activation)\n",
    "def feed_forward_network(x, d_ff=2048):\n",
    "    W1 = np.random.rand(x.shape[-1], d_ff)\n",
    "    W2 = np.random.rand(d_ff, x.shape[-1])\n",
    "    return np.dot(np.maximum(0, np.dot(x, W1)), W2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "728f453f-8ba1-4945-8031-486a68ed497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Layer\n",
    "def bert_layer(x):\n",
    "    # Self-Attention\n",
    "    Q = np.dot(x, np.random.rand(x.shape[-1], x.shape[-1]))\n",
    "    K = np.dot(x, np.random.rand(x.shape[-1], x.shape[-1]))\n",
    "    V = np.dot(x, np.random.rand(x.shape[-1], x.shape[-1]))\n",
    "    attention_output, attention_shape = scaled_dot_product_attention(Q, K, V)\n",
    "    print(\"Attention output shape:\", attention_output.shape)\n",
    "    \n",
    "    # Add & Norm\n",
    "    x = layer_norm(x + attention_output)\n",
    "    \n",
    "    # Feed Forward\n",
    "    ffn_output = feed_forward_network(x)\n",
    "    \n",
    "    # Add & Norm\n",
    "    output = layer_norm(x + ffn_output)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44d5d713-1d36-4251-8ca4-1c56afed8303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output shape: (6, 50)\n"
     ]
    }
   ],
   "source": [
    "output = bert_layer(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87e163ee-14e8-4a51-b269-4be5c6e14622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of BERT Layer:\n",
      " [[-2.80576367e+00 -4.86719916e-01 -3.21210340e-02  1.36941504e+00\n",
      "   2.89200513e-01  1.09584863e+00 -6.10427470e-01 -8.23599205e-01\n",
      "   8.22846676e-01  5.03035577e-01 -5.14814008e-01  1.40426995e-01\n",
      "   1.18915654e+00 -4.08245797e-01  7.34755940e-01  6.03156622e-01\n",
      "   7.63984133e-01 -3.08829341e-01 -1.58614549e+00  2.28037105e-01\n",
      "  -1.60998716e+00 -3.68673263e-01  2.58572723e+00  2.18964181e-02\n",
      "  -6.02041653e-02  5.16157729e-01  2.06537364e-01  7.76772676e-01\n",
      "  -1.58619359e+00  1.17022069e+00 -7.62616718e-01 -8.37639971e-01\n",
      "   2.07845728e+00  2.89141490e-01 -1.17833040e+00 -8.47593705e-01\n",
      "   3.88922455e-01 -3.35521476e-02 -1.49080595e+00 -2.43412394e-01\n",
      "  -1.17262057e-01  7.96336987e-01  1.06720042e+00 -3.66717597e-01\n",
      "  -6.49921172e-01 -5.33928442e-01  7.35249588e-01 -1.40394249e+00\n",
      "   2.53696546e-01  1.04126650e+00]\n",
      " [-2.95349582e+00 -2.40615949e-01 -4.64225821e-02  1.83547285e+00\n",
      "   1.62643151e-01  1.01072096e+00 -3.35986143e-01 -9.56609757e-01\n",
      "   8.76510761e-01  2.14331972e-01 -2.31998007e-01  1.17541154e-01\n",
      "   1.33191073e+00 -3.74026295e-01  7.31731265e-01  5.71469176e-01\n",
      "   6.68508205e-01  6.96816662e-02 -1.47723098e+00  6.97271265e-02\n",
      "  -1.64574903e+00 -3.68047254e-01  2.48417258e+00  1.89945190e-01\n",
      "  -4.93328732e-02  5.32334693e-01  2.46742039e-01  7.72056480e-01\n",
      "  -1.51871592e+00  9.71082962e-01 -1.08622925e+00 -5.61790604e-01\n",
      "   1.89527139e+00  1.01075342e-01 -1.07551681e+00 -1.06647881e+00\n",
      "   4.98213606e-01  1.04318046e-01 -1.43936686e+00 -4.50427924e-01\n",
      "  -4.78006551e-01  5.69632930e-01  1.12203445e+00 -3.51703119e-01\n",
      "  -4.76661051e-01 -6.36767059e-01  8.80697549e-01 -1.37312737e+00\n",
      "   1.38003792e-01  1.02847595e+00]\n",
      " [-2.71315055e+00 -3.96520714e-01 -3.99162581e-02  1.23844290e+00\n",
      "  -4.66749530e-02  8.67754721e-01 -2.67339343e-01 -8.39251596e-01\n",
      "   1.21386000e+00  2.25826048e-01 -2.63780142e-01  2.89853974e-01\n",
      "   1.33291571e+00 -3.47750368e-01  6.66124715e-01  9.08267875e-01\n",
      "   8.72794591e-01 -3.61965854e-02 -1.59715970e+00  2.79281926e-01\n",
      "  -1.88793847e+00 -3.05989698e-01  2.47152644e+00  3.47963683e-01\n",
      "   4.83256580e-02  5.77592068e-01  3.46193388e-01  8.57000818e-01\n",
      "  -1.43922486e+00  1.06620528e+00 -1.19780963e+00 -9.17960485e-01\n",
      "   2.04579656e+00  1.54191713e-01 -1.46670725e+00 -9.04123432e-01\n",
      "   2.72541965e-01 -2.13786528e-02 -1.26204684e+00 -2.92245466e-01\n",
      "  -3.77596229e-01  1.54804300e-01  8.79688936e-01 -3.45270514e-01\n",
      "  -7.16077978e-01 -5.21766703e-01  1.08886238e+00 -1.29447587e+00\n",
      "   3.78122313e-01  9.14414325e-01]\n",
      " [-2.80769397e+00 -2.95623325e-01  1.53301227e-01  1.32867117e+00\n",
      "   2.49186980e-02  1.06710953e+00 -4.94206300e-01 -8.46726776e-01\n",
      "   8.85948321e-01  3.74047537e-01  4.88954347e-02  3.28528147e-01\n",
      "   1.66562042e+00 -2.39342542e-01  4.85894363e-01  6.56629033e-01\n",
      "   1.01867731e+00 -2.81427834e-01 -1.32553558e+00  4.49198276e-02\n",
      "  -1.77516206e+00 -3.20198074e-01  2.39859283e+00  5.63949328e-01\n",
      "   1.30297898e-01  5.25204946e-01 -7.59880932e-02  6.75564383e-01\n",
      "  -1.38399248e+00  1.05258353e+00 -1.08151986e+00 -6.49644601e-01\n",
      "   2.05809580e+00 -1.77378044e-03 -1.41263987e+00 -7.60176884e-01\n",
      "   2.45626781e-01 -2.26820040e-01 -1.64897188e+00 -1.89432887e-01\n",
      "  -3.46091702e-01  3.26053502e-01  9.75647398e-01 -4.90551896e-01\n",
      "  -6.08419150e-01 -6.49031262e-01  8.46451367e-01 -1.44460816e+00\n",
      "   3.77438015e-01  1.09691222e+00]\n",
      " [-2.80175290e+00 -5.84966649e-01 -2.23702585e-01  1.06766011e+00\n",
      "   7.89463165e-02  1.08301895e+00  3.75069911e-02 -1.23123653e+00\n",
      "   1.32084825e+00  8.85248355e-03  1.06077840e-01  1.51833292e-01\n",
      "   1.22571455e+00 -2.16035820e-01  7.94897265e-01  7.87043310e-01\n",
      "   9.68210807e-01 -2.43442737e-01 -1.58717965e+00  2.69738285e-01\n",
      "  -1.60749334e+00 -1.33029955e-01  2.44384577e+00  3.73695687e-01\n",
      "  -8.84021029e-02  4.63786945e-01  2.46743192e-01  7.39987508e-01\n",
      "  -1.55337955e+00  9.89027662e-01 -1.30922430e+00 -2.83364594e-01\n",
      "   1.90268572e+00 -5.96150671e-02 -1.64582288e+00 -6.19048913e-01\n",
      "   1.91031135e-02  3.22691911e-02 -1.43336995e+00 -4.71999691e-01\n",
      "  -4.48012073e-01  5.32528130e-01  1.09727293e+00 -6.90518562e-02\n",
      "  -4.51356589e-01 -5.98295966e-01  1.05173439e+00 -1.30920168e+00\n",
      "   2.25974743e-01  9.49981958e-01]\n",
      " [-2.98997120e+00 -5.33435503e-01 -4.41309692e-01  1.11924214e+00\n",
      "  -1.80288605e-01  1.18290339e+00 -3.35355962e-01 -1.27979676e+00\n",
      "   1.57168821e+00  4.03009143e-02  1.22293543e-01  2.49051048e-02\n",
      "   8.87536058e-01 -8.41564864e-02  8.00974704e-01  8.64028672e-01\n",
      "   8.40866731e-01 -4.77605627e-01 -1.29342842e+00  2.11116189e-01\n",
      "  -1.56650060e+00 -7.85645477e-02  2.26901379e+00  5.56825891e-01\n",
      "  -2.83543070e-01  7.25124218e-01  5.47348542e-01  5.97778966e-01\n",
      "  -1.52213313e+00  1.14599708e+00 -1.04833956e+00 -3.84469381e-01\n",
      "   1.99916531e+00  2.54690455e-01 -9.55541909e-01 -1.22744131e+00\n",
      "   3.07741247e-01  4.56664578e-02 -1.36696739e+00 -4.93300915e-01\n",
      "  -2.34408551e-01  4.74838279e-01  9.73697129e-01 -1.50772049e-01\n",
      "  -5.88598606e-01 -8.08145563e-01  1.03269983e+00 -1.22467143e+00\n",
      "  -1.57786110e-01  1.11008951e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output of BERT Layer:\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
